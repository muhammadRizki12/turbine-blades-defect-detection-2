{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50962ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 16:28:11.876062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750868892.099812      73 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750868892.158421      73 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import (Conv2D, MaxPooling2D, BatchNormalization, Input, Activation)\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eef1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (256, 256)\n",
    "# ROOT_DIR = \"datasets/dataset_split\"\n",
    "ROOT_DIR = \"/kaggle/input/turbine-blades-2/dataset_split/dataset_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008c2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengambil nama-nama kelas dari direktori\n",
    "CLASS_NAMES = ['ablation', 'breakdown', 'fracture', 'groove']\n",
    "\n",
    "# Membuat mapping dari nama kelas ke indeks\n",
    "CLASS_TO_INDEX = {name: index for index, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# Menghitung jumlah kelas\n",
    "NUM_CLASSES = len(CLASS_NAMES) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b832d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe586a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_mask(image_path, mask_path):\n",
    "    # Gambar dibaca, di-decode, dan di-resize (format PNG).\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    # Standardisasi ke rentang 0.0 - 1.0\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "\n",
    "    # Mask dibaca, di-decode, dan di-resize.\n",
    "    msk = tf.io.read_file(mask_path)\n",
    "    msk = tf.image.decode_png(msk, channels=1)\n",
    "    msk = tf.image.resize(msk, IMG_SIZE, method='nearest')\n",
    "    msk = tf.cast(msk, tf.int32)  # pastikan integer\n",
    "    \n",
    "    # Ubah mask ke one-hot sesuai NUM_CLASSES\n",
    "    msk = tf.squeeze(msk, axis=-1)\n",
    "    msk = tf.one_hot(msk, depth=NUM_CLASSES)\n",
    "    return img, msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6311f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(split_dir, batch_size=8, shuffle=True):\n",
    "    img_dir = os.path.join(split_dir, 'images')\n",
    "    mask_dir = os.path.join(split_dir, 'masks')\n",
    "    \n",
    "    img_files = set([\n",
    "        f for f in os.listdir(img_dir)\n",
    "        if not (f.startswith('.') or f.startswith('._')) and f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])\n",
    "    mask_files = set([\n",
    "        f for f in os.listdir(mask_dir)\n",
    "        if not (f.startswith('.') or f.startswith('._')) and f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])\n",
    "    \n",
    "    common_files = sorted(list(img_files & mask_files))\n",
    "    img_paths = [os.path.join(img_dir, f) for f in common_files]\n",
    "    mask_paths = [os.path.join(mask_dir, f) for f in common_files]\n",
    "    print(f\"Jumlah data di {split_dir}: {len(img_paths)}\")\n",
    "    \n",
    "    assert len(img_paths) == len(mask_paths), f\"Jumlah gambar ({len(img_paths)}) dan mask ({len(mask_paths)}) tidak sama!\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, mask_paths))\n",
    "    dataset = dataset.map(load_image_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100, seed=42)\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f83045b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data di /kaggle/input/turbine-blades-2/dataset_split/dataset_split/aug-train: 2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750868929.178777      73 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data di /kaggle/input/turbine-blades-2/dataset_split/dataset_split/val: 227\n",
      "Jumlah data di /kaggle/input/turbine-blades-2/dataset_split/dataset_split/test: 112\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_ds = get_dataset(f'{ROOT_DIR}/aug-train', batch_size=8,shuffle=True)\n",
    "val_ds = get_dataset(f'{ROOT_DIR}/val', batch_size=8, shuffle=False)\n",
    "test_ds = get_dataset(f'{ROOT_DIR}/test', batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ec8d2",
   "metadata": {},
   "source": [
    "# Arsitektur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d2e2d",
   "metadata": {},
   "source": [
    "## Transunet Recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f59ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_block(x, filters, t=3, dropout_rate=0.3):\n",
    "    for i in range(t):\n",
    "        if i == 0:\n",
    "            x1 = layers.Conv2D(filters, kernel_size=3, padding='same', activation='relu')(x)\n",
    "            x1 = layers.SpatialDropout2D(dropout_rate)(x1)  # Dropout ditambahkan setelah Conv2D pertama\n",
    "        else:\n",
    "            x1 = layers.Conv2D(filters, kernel_size=3, padding='same', activation='relu')(layers.Add()([x, x1]))\n",
    "            x1 = layers.BatchNormalization()(x1)\n",
    "            x1 = layers.SpatialDropout2D(dropout_rate)(x1)  # Dropout ditambahkan setelah BatchNorm\n",
    "            return x1\n",
    "\n",
    "def rcnn_block(x, filters, t=3):\n",
    "# Konversi input x ke jumlah channel = filters jika tidak sama\n",
    "    if x.shape[-1] != filters:\n",
    "        x = Conv2D(filters, kernel_size=1, padding='same')(x)\n",
    "        x1 = recurrent_block(x, filters, t)\n",
    "        x2 = recurrent_block(x1, filters, t)\n",
    "        out = layers.Add()([x, x2])\n",
    "        return out\n",
    "\n",
    "def transformer_block(x, num_heads=4, ff_dim=512):\n",
    "    B, H, W, C = x.shape\n",
    "    x_flat = layers.Reshape((H * W, C))(x)\n",
    "    \n",
    "    # LayerNorm + MHA\n",
    "    attn_input = layers.LayerNormalization(epsilon=1e-6)(x_flat)\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=C // num_heads)(attn_input, attn_input)\n",
    "    x = x_flat + attn_output  # Residual connection\n",
    "\n",
    "    # FFN   \n",
    "    ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    ffn_output = layers.Dense(ff_dim, activation='relu')(ffn_input)\n",
    "    ffn_output = layers.Dense(C)(ffn_output)\n",
    "    x = x + ffn_output  # Residual connection\n",
    "\n",
    "    return layers.Reshape((H, W, C))(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81641f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transunet_with_rcnn(input_size=(256, 256, 3), num_classes=NUM_CLASSES):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = rcnn_block(inputs, 32)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = rcnn_block(pool1, 64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = rcnn_block(pool2, 128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = rcnn_block(pool3, 256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    # Bottleneck with Transformer\n",
    "    conv5 = rcnn_block(pool4, 512)\n",
    "    trans = transformer_block(conv5)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = layers.concatenate([layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(trans), conv4], axis=-1)\n",
    "    conv6 = rcnn_block(up6, 256)\n",
    "\n",
    "    up7 = layers.concatenate([layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=-1)\n",
    "    conv7 = rcnn_block(up7, 128)\n",
    "\n",
    "    up8 = layers.concatenate([layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=-1)\n",
    "    conv8 = rcnn_block(up8, 64)\n",
    "\n",
    "    up9 = layers.concatenate([layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=-1)\n",
    "    conv9 = rcnn_block(up9, 32)\n",
    "\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(conv9)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, name=\"Transunet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0fd8a",
   "metadata": {},
   "source": [
    "# Metrix dan Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b22b10",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64957c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_dsc', patience=10, mode='max',\n",
    "                              verbose=1, restore_best_weights=True, min_delta=1e-6)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_dsc', factor=0.5, patience=3, mode='max', verbose=2)\n",
    "\n",
    "transunet_checkpoint = ModelCheckpoint(\n",
    "    filepath='model/transunet.keras',\n",
    "    monitor='val_dsc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a344ef0",
   "metadata": {},
   "source": [
    "## Metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0e2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "IMG_SIZE = (256, 256)  # atau sesuai ukuran kamu\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a8827",
   "metadata": {},
   "source": [
    "## IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ab58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred, smooth=1e-7):\n",
    "    y_true_f = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n",
    "    union = tf.reduce_sum(y_true_f + y_pred_f, axis=0) - intersection\n",
    "\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc9e78",
   "metadata": {},
   "source": [
    "### Dice Coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58a9b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsc(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Multi-class Dice Coefficient.\n",
    "    y_true, y_pred: shape (batch, H, W, C), one-hot encoded.\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[0, 1, 2])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[0, 1, 2])\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8244b",
   "metadata": {},
   "source": [
    "### Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "893c6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dsc(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2b2f3",
   "metadata": {},
   "source": [
    "### BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ab59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f0232",
   "metadata": {},
   "source": [
    "### Tversky loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ba82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tversky_loss(y_true, y_pred, alpha=0.7, beta=0.3):\n",
    "    y_true_pos =  tf.keras.backend.flatten(y_true)\n",
    "    y_pred_pos =  tf.keras.backend.flatten(y_pred)\n",
    "    true_pos =  tf.keras.backend.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg =  tf.keras.backend.sum(y_true_pos * (1 - y_pred_pos))\n",
    "    false_pos =  tf.keras.backend.sum((1 - y_true_pos) * y_pred_pos)\n",
    "    return 1 - (true_pos + 1e-6) / (true_pos + alpha * false_neg + beta * false_pos + 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcce82b",
   "metadata": {},
   "source": [
    "### Weigted + Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6968de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_WEIGHTS = [0.05, 1.5, 2.0, 3.0, 4.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b18d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)  # untuk menghindari log(0)\n",
    "        loss = y_true * tf.math.log(y_pred) * weights\n",
    "        return -tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[1, 2, 3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "def combined_loss(weights, alpha=0.5):\n",
    "    \"\"\"\n",
    "    weights: list of class weights (e.g., [0.1, 2.0, 2.5, 3.0, 4.0])\n",
    "    alpha: balance factor between crossentropy and dice loss\n",
    "    \"\"\"\n",
    "    wce = weighted_categorical_crossentropy(weights)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        return alpha * wce(y_true, y_pred) + (1 - alpha) * dice_loss(y_true, y_pred)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66256c0",
   "metadata": {},
   "source": [
    "### Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12e92a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_tversky_loss(y_true, y_pred, alpha=0.7, beta=0.3, gamma=0.75, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    tp = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred), axis=[1,2,3])\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred, axis=[1,2,3])\n",
    "    \n",
    "    tversky = (tp + smooth) / (tp + alpha*fn + beta*fp + smooth)\n",
    "    return tf.reduce_mean(tf.pow((1 - tversky), gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ce4c9",
   "metadata": {},
   "source": [
    "### Focal tversky + weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9602bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda y_true, y_pred: 0.4 * weighted_categorical_crossentropy(CLASS_WEIGHTS)(y_true, y_pred) + 0.6 * focal_tversky_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b114c1",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96bed0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transunet_unet = transunet_with_rcnn()\n",
    "model_transunet_unet.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    # loss=categorical_crossentropy,\n",
    "    # loss=bce_dice_loss,\n",
    "    # loss=loss_fn,\n",
    "    loss=combined_loss(CLASS_WEIGHTS, alpha=0.4),\n",
    "    # loss=focal_tversky_loss,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        dsc,\n",
    "        iou\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99456542",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0fa10d",
   "metadata": {},
   "source": [
    "## Transformer Unet Recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05831b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 209ms/step - accuracy: 0.6980 - dsc: 0.1937 - iou: 0.1565 - loss: 0.3201 - val_accuracy: 0.9927 - val_dsc: 0.1993 - val_iou: 0.1986 - val_loss: 0.0841 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9789 - dsc: 0.2018 - iou: 0.1958 - loss: 0.0960 - val_accuracy: 0.9927 - val_dsc: 0.2001 - val_iou: 0.1988 - val_loss: 0.0655 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 178ms/step - accuracy: 0.9823 - dsc: 0.2048 - iou: 0.1976 - loss: 0.0821 - val_accuracy: 0.9927 - val_dsc: 0.2007 - val_iou: 0.1986 - val_loss: 0.0517 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9839 - dsc: 0.2061 - iou: 0.1986 - loss: 0.0702 - val_accuracy: 0.9927 - val_dsc: 0.2004 - val_iou: 0.1988 - val_loss: 0.0543 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9842 - dsc: 0.2056 - iou: 0.1980 - loss: 0.0732 - val_accuracy: 0.9927 - val_dsc: 0.2004 - val_iou: 0.1988 - val_loss: 0.0513 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9870 - dsc: 0.2065 - iou: 0.1991 - loss: 0.0678 - val_accuracy: 0.9927 - val_dsc: 0.2008 - val_iou: 0.1990 - val_loss: 0.0555 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9875 - dsc: 0.2080 - iou: 0.2001 - loss: 0.0627 - val_accuracy: 0.9927 - val_dsc: 0.2006 - val_iou: 0.1989 - val_loss: 0.0528 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9866 - dsc: 0.2089 - iou: 0.2004 - loss: 0.0654 - val_accuracy: 0.9927 - val_dsc: 0.2000 - val_iou: 0.1987 - val_loss: 0.0540 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.9874 - dsc: 0.2103 - iou: 0.2015 - loss: 0.0616\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9874 - dsc: 0.2103 - iou: 0.2015 - loss: 0.0616 - val_accuracy: 0.9927 - val_dsc: 0.1999 - val_iou: 0.1986 - val_loss: 0.0552 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9876 - dsc: 0.2098 - iou: 0.2013 - loss: 0.0623 - val_accuracy: 0.9927 - val_dsc: 0.2007 - val_iou: 0.1982 - val_loss: 0.0496 - learning_rate: 5.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9873 - dsc: 0.2119 - iou: 0.2025 - loss: 0.0588 - val_accuracy: 0.9927 - val_dsc: 0.2001 - val_iou: 0.1983 - val_loss: 0.0559 - learning_rate: 5.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9868 - dsc: 0.2127 - iou: 0.2029 - loss: 0.0616 - val_accuracy: 0.9927 - val_dsc: 0.2014 - val_iou: 0.1989 - val_loss: 0.0494 - learning_rate: 5.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9872 - dsc: 0.2132 - iou: 0.2034 - loss: 0.0572 - val_accuracy: 0.9927 - val_dsc: 0.2010 - val_iou: 0.1991 - val_loss: 0.0523 - learning_rate: 5.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 176ms/step - accuracy: 0.9870 - dsc: 0.2141 - iou: 0.2038 - loss: 0.0575 - val_accuracy: 0.9927 - val_dsc: 0.2013 - val_iou: 0.1991 - val_loss: 0.0497 - learning_rate: 5.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 178ms/step - accuracy: 0.9867 - dsc: 0.2166 - iou: 0.2053 - loss: 0.0559 - val_accuracy: 0.9927 - val_dsc: 0.2016 - val_iou: 0.1993 - val_loss: 0.0472 - learning_rate: 5.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9868 - dsc: 0.2168 - iou: 0.2054 - loss: 0.0562 - val_accuracy: 0.9927 - val_dsc: 0.2018 - val_iou: 0.1995 - val_loss: 0.0462 - learning_rate: 5.0000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9866 - dsc: 0.2198 - iou: 0.2073 - loss: 0.0540 - val_accuracy: 0.9927 - val_dsc: 0.2004 - val_iou: 0.1987 - val_loss: 0.0521 - learning_rate: 5.0000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 178ms/step - accuracy: 0.9868 - dsc: 0.2191 - iou: 0.2069 - loss: 0.0537 - val_accuracy: 0.9927 - val_dsc: 0.2011 - val_iou: 0.1990 - val_loss: 0.0467 - learning_rate: 5.0000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.9866 - dsc: 0.2224 - iou: 0.2088 - loss: 0.0532\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9866 - dsc: 0.2224 - iou: 0.2088 - loss: 0.0531 - val_accuracy: 0.9927 - val_dsc: 0.2009 - val_iou: 0.1990 - val_loss: 0.0477 - learning_rate: 5.0000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9871 - dsc: 0.2199 - iou: 0.2076 - loss: 0.0541 - val_accuracy: 0.9927 - val_dsc: 0.2016 - val_iou: 0.1992 - val_loss: 0.0480 - learning_rate: 2.5000e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9873 - dsc: 0.2217 - iou: 0.2087 - loss: 0.0524 - val_accuracy: 0.9926 - val_dsc: 0.2019 - val_iou: 0.1993 - val_loss: 0.0451 - learning_rate: 2.5000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9867 - dsc: 0.2242 - iou: 0.2101 - loss: 0.0518 - val_accuracy: 0.9927 - val_dsc: 0.2003 - val_iou: 0.1985 - val_loss: 0.0489 - learning_rate: 2.5000e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9872 - dsc: 0.2245 - iou: 0.2103 - loss: 0.0504 - val_accuracy: 0.9927 - val_dsc: 0.2012 - val_iou: 0.1991 - val_loss: 0.0462 - learning_rate: 2.5000e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.9868 - dsc: 0.2251 - iou: 0.2107 - loss: 0.0500\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9868 - dsc: 0.2251 - iou: 0.2107 - loss: 0.0500 - val_accuracy: 0.9927 - val_dsc: 0.2006 - val_iou: 0.1990 - val_loss: 0.0479 - learning_rate: 2.5000e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9878 - dsc: 0.2219 - iou: 0.2093 - loss: 0.0505 - val_accuracy: 0.9922 - val_dsc: 0.2034 - val_iou: 0.2002 - val_loss: 0.0425 - learning_rate: 1.2500e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9871 - dsc: 0.2259 - iou: 0.2116 - loss: 0.0505 - val_accuracy: 0.9923 - val_dsc: 0.2032 - val_iou: 0.2002 - val_loss: 0.0417 - learning_rate: 1.2500e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9874 - dsc: 0.2263 - iou: 0.2120 - loss: 0.0492 - val_accuracy: 0.9923 - val_dsc: 0.2038 - val_iou: 0.2005 - val_loss: 0.0421 - learning_rate: 1.2500e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9875 - dsc: 0.2256 - iou: 0.2114 - loss: 0.0483 - val_accuracy: 0.9922 - val_dsc: 0.2030 - val_iou: 0.2000 - val_loss: 0.0416 - learning_rate: 1.2500e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9872 - dsc: 0.2273 - iou: 0.2125 - loss: 0.0474 - val_accuracy: 0.9925 - val_dsc: 0.2032 - val_iou: 0.2001 - val_loss: 0.0419 - learning_rate: 1.2500e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.9873 - dsc: 0.2277 - iou: 0.2127 - loss: 0.0481\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9873 - dsc: 0.2277 - iou: 0.2127 - loss: 0.0480 - val_accuracy: 0.9926 - val_dsc: 0.2029 - val_iou: 0.2001 - val_loss: 0.0425 - learning_rate: 1.2500e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 178ms/step - accuracy: 0.9880 - dsc: 0.2259 - iou: 0.2121 - loss: 0.0486 - val_accuracy: 0.9917 - val_dsc: 0.2062 - val_iou: 0.2014 - val_loss: 0.0384 - learning_rate: 6.2500e-06\n",
      "Epoch 32/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9879 - dsc: 0.2295 - iou: 0.2141 - loss: 0.0453 - val_accuracy: 0.9915 - val_dsc: 0.2061 - val_iou: 0.2013 - val_loss: 0.0384 - learning_rate: 6.2500e-06\n",
      "Epoch 33/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9875 - dsc: 0.2317 - iou: 0.2155 - loss: 0.0461 - val_accuracy: 0.9913 - val_dsc: 0.2069 - val_iou: 0.2017 - val_loss: 0.0378 - learning_rate: 6.2500e-06\n",
      "Epoch 34/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9875 - dsc: 0.2303 - iou: 0.2145 - loss: 0.0455 - val_accuracy: 0.9915 - val_dsc: 0.2064 - val_iou: 0.2016 - val_loss: 0.0380 - learning_rate: 6.2500e-06\n",
      "Epoch 35/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9879 - dsc: 0.2311 - iou: 0.2152 - loss: 0.0441 - val_accuracy: 0.9914 - val_dsc: 0.2065 - val_iou: 0.2016 - val_loss: 0.0380 - learning_rate: 6.2500e-06\n",
      "Epoch 36/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9877 - dsc: 0.2318 - iou: 0.2156 - loss: 0.0445\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9877 - dsc: 0.2318 - iou: 0.2156 - loss: 0.0445 - val_accuracy: 0.9916 - val_dsc: 0.2068 - val_iou: 0.2018 - val_loss: 0.0379 - learning_rate: 6.2500e-06\n",
      "Epoch 37/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9879 - dsc: 0.2300 - iou: 0.2148 - loss: 0.0453 - val_accuracy: 0.9905 - val_dsc: 0.2084 - val_iou: 0.2023 - val_loss: 0.0373 - learning_rate: 3.1250e-06\n",
      "Epoch 38/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9876 - dsc: 0.2328 - iou: 0.2161 - loss: 0.0438 - val_accuracy: 0.9903 - val_dsc: 0.2091 - val_iou: 0.2027 - val_loss: 0.0371 - learning_rate: 3.1250e-06\n",
      "Epoch 39/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9875 - dsc: 0.2325 - iou: 0.2160 - loss: 0.0443 - val_accuracy: 0.9898 - val_dsc: 0.2091 - val_iou: 0.2026 - val_loss: 0.0375 - learning_rate: 3.1250e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9873 - dsc: 0.2324 - iou: 0.2158 - loss: 0.0440 - val_accuracy: 0.9903 - val_dsc: 0.2089 - val_iou: 0.2026 - val_loss: 0.0373 - learning_rate: 3.1250e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9875 - dsc: 0.2332 - iou: 0.2167 - loss: 0.0439\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9875 - dsc: 0.2332 - iou: 0.2167 - loss: 0.0439 - val_accuracy: 0.9906 - val_dsc: 0.2089 - val_iou: 0.2027 - val_loss: 0.0370 - learning_rate: 3.1250e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9872 - dsc: 0.2338 - iou: 0.2168 - loss: 0.0454 - val_accuracy: 0.9897 - val_dsc: 0.2102 - val_iou: 0.2032 - val_loss: 0.0371 - learning_rate: 1.5625e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9874 - dsc: 0.2351 - iou: 0.2176 - loss: 0.0428 - val_accuracy: 0.9896 - val_dsc: 0.2102 - val_iou: 0.2031 - val_loss: 0.0370 - learning_rate: 1.5625e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9871 - dsc: 0.2338 - iou: 0.2168 - loss: 0.0444 - val_accuracy: 0.9898 - val_dsc: 0.2107 - val_iou: 0.2034 - val_loss: 0.0366 - learning_rate: 1.5625e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9872 - dsc: 0.2354 - iou: 0.2178 - loss: 0.0429 - val_accuracy: 0.9897 - val_dsc: 0.2105 - val_iou: 0.2033 - val_loss: 0.0369 - learning_rate: 1.5625e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9873 - dsc: 0.2353 - iou: 0.2180 - loss: 0.0427 - val_accuracy: 0.9897 - val_dsc: 0.2109 - val_iou: 0.2036 - val_loss: 0.0368 - learning_rate: 1.5625e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9871 - dsc: 0.2343 - iou: 0.2170 - loss: 0.0436 - val_accuracy: 0.9896 - val_dsc: 0.2113 - val_iou: 0.2038 - val_loss: 0.0367 - learning_rate: 1.5625e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9870 - dsc: 0.2339 - iou: 0.2168 - loss: 0.0435 - val_accuracy: 0.9899 - val_dsc: 0.2105 - val_iou: 0.2034 - val_loss: 0.0369 - learning_rate: 1.5625e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9872 - dsc: 0.2356 - iou: 0.2178 - loss: 0.0431 - val_accuracy: 0.9900 - val_dsc: 0.2107 - val_iou: 0.2035 - val_loss: 0.0367 - learning_rate: 1.5625e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9873 - dsc: 0.2357 - iou: 0.2180 - loss: 0.0427\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9873 - dsc: 0.2357 - iou: 0.2180 - loss: 0.0426 - val_accuracy: 0.9902 - val_dsc: 0.2106 - val_iou: 0.2035 - val_loss: 0.0366 - learning_rate: 1.5625e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 175ms/step - accuracy: 0.9872 - dsc: 0.2347 - iou: 0.2175 - loss: 0.0428 - val_accuracy: 0.9900 - val_dsc: 0.2106 - val_iou: 0.2034 - val_loss: 0.0368 - learning_rate: 7.8125e-07\n",
      "Epoch 52/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 175ms/step - accuracy: 0.9871 - dsc: 0.2343 - iou: 0.2171 - loss: 0.0441 - val_accuracy: 0.9899 - val_dsc: 0.2109 - val_iou: 0.2036 - val_loss: 0.0366 - learning_rate: 7.8125e-07\n",
      "Epoch 53/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.9866 - dsc: 0.2363 - iou: 0.2179 - loss: 0.0448\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 175ms/step - accuracy: 0.9866 - dsc: 0.2363 - iou: 0.2179 - loss: 0.0448 - val_accuracy: 0.9898 - val_dsc: 0.2111 - val_iou: 0.2037 - val_loss: 0.0366 - learning_rate: 7.8125e-07\n",
      "Epoch 54/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 175ms/step - accuracy: 0.9870 - dsc: 0.2343 - iou: 0.2171 - loss: 0.0428 - val_accuracy: 0.9899 - val_dsc: 0.2109 - val_iou: 0.2036 - val_loss: 0.0367 - learning_rate: 3.9062e-07\n",
      "Epoch 55/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 175ms/step - accuracy: 0.9873 - dsc: 0.2347 - iou: 0.2173 - loss: 0.0425 - val_accuracy: 0.9898 - val_dsc: 0.2111 - val_iou: 0.2037 - val_loss: 0.0367 - learning_rate: 3.9062e-07\n",
      "Epoch 56/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.9871 - dsc: 0.2342 - iou: 0.2171 - loss: 0.0422\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 175ms/step - accuracy: 0.9871 - dsc: 0.2342 - iou: 0.2171 - loss: 0.0422 - val_accuracy: 0.9898 - val_dsc: 0.2112 - val_iou: 0.2037 - val_loss: 0.0367 - learning_rate: 3.9062e-07\n",
      "Epoch 57/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 177ms/step - accuracy: 0.9871 - dsc: 0.2367 - iou: 0.2185 - loss: 0.0419 - val_accuracy: 0.9898 - val_dsc: 0.2115 - val_iou: 0.2039 - val_loss: 0.0366 - learning_rate: 1.9531e-07\n",
      "Epoch 58/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9869 - dsc: 0.2348 - iou: 0.2173 - loss: 0.0428 - val_accuracy: 0.9899 - val_dsc: 0.2111 - val_iou: 0.2037 - val_loss: 0.0366 - learning_rate: 1.9531e-07\n",
      "Epoch 59/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9870 - dsc: 0.2356 - iou: 0.2178 - loss: 0.0425 - val_accuracy: 0.9898 - val_dsc: 0.2114 - val_iou: 0.2039 - val_loss: 0.0365 - learning_rate: 1.9531e-07\n",
      "Epoch 60/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9874 - dsc: 0.2364 - iou: 0.2183 - loss: 0.0421\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9874 - dsc: 0.2364 - iou: 0.2183 - loss: 0.0420 - val_accuracy: 0.9898 - val_dsc: 0.2112 - val_iou: 0.2038 - val_loss: 0.0367 - learning_rate: 1.9531e-07\n",
      "Epoch 61/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9874 - dsc: 0.2357 - iou: 0.2179 - loss: 0.0410 - val_accuracy: 0.9899 - val_dsc: 0.2112 - val_iou: 0.2037 - val_loss: 0.0366 - learning_rate: 9.7656e-08\n",
      "Epoch 62/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9869 - dsc: 0.2348 - iou: 0.2174 - loss: 0.0425 - val_accuracy: 0.9898 - val_dsc: 0.2113 - val_iou: 0.2038 - val_loss: 0.0366 - learning_rate: 9.7656e-08\n",
      "Epoch 63/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9867 - dsc: 0.2352 - iou: 0.2175 - loss: 0.0433\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9867 - dsc: 0.2352 - iou: 0.2174 - loss: 0.0433 - val_accuracy: 0.9897 - val_dsc: 0.2114 - val_iou: 0.2039 - val_loss: 0.0367 - learning_rate: 9.7656e-08\n",
      "Epoch 64/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9869 - dsc: 0.2351 - iou: 0.2174 - loss: 0.0426 - val_accuracy: 0.9899 - val_dsc: 0.2115 - val_iou: 0.2039 - val_loss: 0.0367 - learning_rate: 4.8828e-08\n",
      "Epoch 65/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9868 - dsc: 0.2360 - iou: 0.2180 - loss: 0.0420 - val_accuracy: 0.9898 - val_dsc: 0.2115 - val_iou: 0.2039 - val_loss: 0.0366 - learning_rate: 4.8828e-08\n",
      "Epoch 66/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9872 - dsc: 0.2361 - iou: 0.2180 - loss: 0.0415\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9872 - dsc: 0.2360 - iou: 0.2180 - loss: 0.0415 - val_accuracy: 0.9897 - val_dsc: 0.2115 - val_iou: 0.2039 - val_loss: 0.0368 - learning_rate: 4.8828e-08\n",
      "Epoch 67/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 177ms/step - accuracy: 0.9867 - dsc: 0.2367 - iou: 0.2184 - loss: 0.0431 - val_accuracy: 0.9897 - val_dsc: 0.2118 - val_iou: 0.2041 - val_loss: 0.0365 - learning_rate: 2.4414e-08\n",
      "Epoch 68/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9869 - dsc: 0.2359 - iou: 0.2181 - loss: 0.0428 - val_accuracy: 0.9897 - val_dsc: 0.2114 - val_iou: 0.2038 - val_loss: 0.0367 - learning_rate: 2.4414e-08\n",
      "Epoch 69/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9872 - dsc: 0.2378 - iou: 0.2190 - loss: 0.0421 - val_accuracy: 0.9898 - val_dsc: 0.2117 - val_iou: 0.2041 - val_loss: 0.0364 - learning_rate: 2.4414e-08\n",
      "Epoch 70/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9867 - dsc: 0.2345 - iou: 0.2171 - loss: 0.0432\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9867 - dsc: 0.2345 - iou: 0.2171 - loss: 0.0431 - val_accuracy: 0.9897 - val_dsc: 0.2116 - val_iou: 0.2039 - val_loss: 0.0366 - learning_rate: 2.4414e-08\n",
      "Epoch 71/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9870 - dsc: 0.2347 - iou: 0.2173 - loss: 0.0424 - val_accuracy: 0.9898 - val_dsc: 0.2117 - val_iou: 0.2040 - val_loss: 0.0365 - learning_rate: 1.2207e-08\n",
      "Epoch 72/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9869 - dsc: 0.2365 - iou: 0.2182 - loss: 0.0426 - val_accuracy: 0.9898 - val_dsc: 0.2116 - val_iou: 0.2040 - val_loss: 0.0366 - learning_rate: 1.2207e-08\n",
      "Epoch 73/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9866 - dsc: 0.2349 - iou: 0.2172 - loss: 0.0434\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9866 - dsc: 0.2348 - iou: 0.2172 - loss: 0.0434 - val_accuracy: 0.9898 - val_dsc: 0.2116 - val_iou: 0.2040 - val_loss: 0.0365 - learning_rate: 1.2207e-08\n",
      "Epoch 74/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9870 - dsc: 0.2357 - iou: 0.2178 - loss: 0.0426 - val_accuracy: 0.9898 - val_dsc: 0.2117 - val_iou: 0.2040 - val_loss: 0.0363 - learning_rate: 6.1035e-09\n",
      "Epoch 75/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9871 - dsc: 0.2351 - iou: 0.2175 - loss: 0.0430 - val_accuracy: 0.9897 - val_dsc: 0.2114 - val_iou: 0.2039 - val_loss: 0.0367 - learning_rate: 6.1035e-09\n",
      "Epoch 76/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9868 - dsc: 0.2354 - iou: 0.2177 - loss: 0.0424\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9869 - dsc: 0.2354 - iou: 0.2177 - loss: 0.0423 - val_accuracy: 0.9899 - val_dsc: 0.2114 - val_iou: 0.2038 - val_loss: 0.0365 - learning_rate: 6.1035e-09\n",
      "Epoch 77/100\n",
      "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 176ms/step - accuracy: 0.9872 - dsc: 0.2362 - iou: 0.2182 - loss: 0.0419 - val_accuracy: 0.9900 - val_dsc: 0.2115 - val_iou: 0.2039 - val_loss: 0.0366 - learning_rate: 3.0518e-09\n",
      "Epoch 77: early stopping\n",
      "Restoring model weights from the end of the best epoch: 67.\n"
     ]
    }
   ],
   "source": [
    "history_transunet_unet = model_transunet_unet.fit(train_ds, validation_data=val_ds,\n",
    "                    epochs=EPOCHS, callbacks=[earlystopping, reduce_lr], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a228bfe",
   "metadata": {},
   "source": [
    "# Grafik Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val accuracy')\n",
    "    plt.title(f\"{title} - accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Dice Coefficient\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['dsc'], label='Train dsc')\n",
    "    plt.plot(history.history['val_dsc'], label='Validation dsc')\n",
    "    plt.title(f\"{title} - Dice Coefficient\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('DSC')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in [\n",
    "                history_transunet_unet]: \n",
    "    plot_training_history(history, title=history.model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e1cc6",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6920c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_per_class(model, dataset, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Menghitung Mean IoU per kelas pada seluruh dataset.\n",
    "    \"\"\"\n",
    "    total_intersection = np.zeros(num_classes)\n",
    "    total_union = np.zeros(num_classes)\n",
    "\n",
    "    for images, masks in dataset:\n",
    "        preds = model.predict(images)\n",
    "        y_pred = np.argmax(preds, axis=-1).reshape(-1)\n",
    "        y_true = np.argmax(masks.numpy(), axis=-1).reshape(-1)  # or masks.numpy().reshape(-1) if already in int format\n",
    "        for c in range(num_classes):\n",
    "            true_c = (y_true == c)\n",
    "            pred_c = (y_pred == c)\n",
    "            intersection = np.logical_and(true_c, pred_c).sum()\n",
    "            union = np.logical_or(true_c, pred_c).sum()\n",
    "            total_intersection[c] += intersection\n",
    "            total_union[c] += union\n",
    "\n",
    "    mean_iou = np.divide(\n",
    "        total_intersection,\n",
    "        total_union,\n",
    "        out=np.zeros_like(total_intersection, dtype=np.float32),\n",
    "        where=total_union != 0\n",
    "    )\n",
    "    return mean_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_transunet_unet.evaluate(test_ds)\n",
    "print(\"Test loss:\", results[0])\n",
    "print(\"Test categorical_accuracyt:\", results[1])\n",
    "print(\"Test dice_coef:\", results[2])\n",
    "print(\"Test iou:\", results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_rec_unet.evaluate(test_ds)\n",
    "print(\"Test loss:\", results[0])\n",
    "print(\"Test categorical_accuracyt:\", results[1])\n",
    "print(\"Test dice_coef:\", results[2])\n",
    "print(\"Test iou:\", results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_resnet50_unet.evaluate(test_ds)\n",
    "print(\"Test loss:\", results[0])\n",
    "print(\"Test categorical_accuracyt:\", results[1])\n",
    "print(\"Test dice_coef:\", results[2])\n",
    "print(\"Test miou:\", results[3])\n",
    "# print(\"Test iou_score:\", results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_attention_unet.evaluate(test_ds)\n",
    "print(\"Test loss:\", results[0])\n",
    "print(\"Test categorical_accuracyt:\", results[1])\n",
    "print(\"Test dice_coef:\", results[2])\n",
    "print(\"Test miou:\", results[3])\n",
    "# print(\"Test iou_score:\", results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAPPING = {\n",
    "    0: 'Background',\n",
    "    1: 'ablation',\n",
    "    2: 'breakdown',\n",
    "    3: 'fracture',\n",
    "    4: 'groove'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh penggunaan:\n",
    "miou_per_class = mean_iou_per_class(model_transunet_unet, test_ds, num_classes=NUM_CLASSES)\n",
    "for idx, name in CLASS_MAPPING.items():\n",
    "    print(f\"Mean IoU for class '{name}': {miou_per_class[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f10b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh penggunaan:\n",
    "miou_per_class = mean_iou_per_class(model_resnet50_unet, test_ds, num_classes=NUM_CLASSES)\n",
    "for idx, name in CLASS_MAPPING.items():\n",
    "    print(f\"Mean IoU for class '{name}': {miou_per_class[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh penggunaan:\n",
    "miou_per_class = mean_iou_per_class(model_attention_unet, test_ds, num_classes=NUM_CLASSES)\n",
    "for idx, name in CLASS_MAPPING.items():\n",
    "    print(f\"Mean IoU for class '{name}': {miou_per_class[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcedc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model_and_plot_confusion(model, dataset, class_names):\n",
    "    \"\"\"\n",
    "    Evaluasi model segmentasi:\n",
    "    - Menampilkan test loss, accuracy, dice coef, dan mIoU\n",
    "    - Menampilkan confusion matrix pixel-wise\n",
    "    \"\"\"\n",
    "\n",
    "    # Hitung confusion matrix pixel-wise\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for images, masks in dataset:\n",
    "        preds = model.predict(images)\n",
    "        y_pred = np.argmax(preds, axis=-1).flatten()\n",
    "        y_true = np.argmax(masks.numpy(), axis=-1).flatten()\n",
    "        y_true_all.extend(y_true)\n",
    "        y_pred_all.extend(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_true_all, y_pred_all, labels=range(len(class_names)))\n",
    "\n",
    "    # Visualisasi confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix (Pixel-wise)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "      # Evaluasi model\n",
    "    print(classification_report(\n",
    "        y_true_all, y_pred_all,\n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d91bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Background', 'ablation', 'breakdown', 'fracture', 'groove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d022a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_and_plot_confusion(model_transunet_unet, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a00613",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_and_plot_confusion(model_resnet50_unet, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e98f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(split_dir):\n",
    "    mask_dir = os.path.join(split_dir, 'masks')\n",
    "    mask_files = [f for f in os.listdir(mask_dir) if f.endswith('.png')]\n",
    "    unique_labels = set()\n",
    "\n",
    "    for f in mask_files:\n",
    "        path = os.path.join(mask_dir, f)\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_png(img, channels=1)\n",
    "        unique = tf.unique(tf.reshape(img, [-1]))[0].numpy()\n",
    "        unique_labels.update(unique.tolist())\n",
    "    \n",
    "    print(f\"Label yang ditemukan di {split_dir}:\", sorted(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_labels('datasets/dataset_split/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3403e05",
   "metadata": {},
   "source": [
    "# Visualisasi testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask_on_image(image, mask, class_names, alpha=0.5):\n",
    "    \"\"\"\n",
    "    image: numpy array (H, W, 3), range [0, 1] or [0, 255]\n",
    "    mask: numpy array (H, W), berisi index kelas (0=background, dst)\n",
    "    class_names: list of str\n",
    "    alpha: transparansi overlay\n",
    "    \"\"\"\n",
    "    # Pastikan image dalam range [0, 1]\n",
    "    if image.max() > 1.0:\n",
    "        image = image / 255.0\n",
    "\n",
    "    # Buat colormap\n",
    "    cmap = plt.get_cmap('tab20', len(class_names))\n",
    "    mask_rgb = cmap(mask)[..., :3]  # Ambil RGB saja\n",
    "\n",
    "    # Overlay: background (0) tidak diwarnai\n",
    "    mask_bool = mask > 0\n",
    "    overlay = image.copy()\n",
    "    overlay[mask_bool] = (1 - alpha) * image[mask_bool] + alpha * mask_rgb[mask_bool]\n",
    "    return overlay\n",
    "\n",
    "# Contoh penggunaan di fungsi predict_and_visualize:\n",
    "def predict_and_visualize(model, image_path, input_size=(256, 256), class_names=None):\n",
    "    img = load_img(image_path, target_size=input_size)\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_input = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    pred_mask = model.predict(img_input)[0]  # shape: (H, W, C)\n",
    "    mask_argmax = np.argmax(pred_mask, axis=-1)  # shape: (H, W)\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Gambar asli\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Mask prediksi\n",
    "    plt.subplot(1, 3, 2)\n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class {i}\" for i in range(pred_mask.shape[-1])]\n",
    "    cmap = plt.get_cmap('tab20', len(class_names))\n",
    "    im = plt.imshow(mask_argmax, cmap=cmap, vmin=0, vmax=len(class_names)-1)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    cbar = plt.colorbar(im, ticks=range(len(class_names)), fraction=0.046, pad=0.04)\n",
    "    cbar.ax.set_yticklabels(class_names)\n",
    "    plt.clim(-0.5, len(class_names)-0.5)\n",
    "\n",
    "    # Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    overlay = overlay_mask_on_image(img_array, mask_argmax, class_names, alpha=0.5)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e243db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Overlay mask ke atas gambar asli\n",
    "def overlay_mask_on_image(image, mask, class_names, alpha=0.5):\n",
    "    if image.max() > 1.0:\n",
    "        image = image / 255.0\n",
    "\n",
    "    cmap = plt.get_cmap('tab20', len(class_names))\n",
    "    mask_rgb = cmap(mask)[..., :3]\n",
    "\n",
    "    mask_bool = mask > 0\n",
    "    overlay = image.copy()\n",
    "    overlay[mask_bool] = (1 - alpha) * image[mask_bool] + alpha * mask_rgb[mask_bool]\n",
    "    return overlay\n",
    "\n",
    "# Visualisasi image + mask (true), pred mask, image + pred mask\n",
    "def visualize_segmentation(image_array, pred_mask, true_mask=None, class_names=None):\n",
    "    if class_names is None:\n",
    "        num_classes = pred_mask.max() + 1 if true_mask is None else max(pred_mask.max(), true_mask.max()) + 1\n",
    "        class_names = [f\"Class {i}\" for i in range(num_classes)]\n",
    "\n",
    "    cmap = plt.get_cmap('tab20', len(class_names))\n",
    "    ncols = 3 if true_mask is not None else 2\n",
    "    plt.figure(figsize=(6 * ncols, 6))\n",
    "\n",
    "    if true_mask is not None:\n",
    "        plt.subplot(1, ncols, 1)\n",
    "        overlay_true = overlay_mask_on_image(image_array, true_mask, class_names, alpha=0.5)\n",
    "        plt.imshow(overlay_true)\n",
    "        plt.title(\"Image + True Mask\")\n",
    "        plt.axis(\"off\")\n",
    "        idx = 2\n",
    "    else:\n",
    "        idx = 1\n",
    "\n",
    "    plt.subplot(1, ncols, idx)\n",
    "    im = plt.imshow(pred_mask, cmap=cmap, vmin=0, vmax=len(class_names)-1)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    cbar = plt.colorbar(im, ticks=range(len(class_names)), fraction=0.046, pad=0.04)\n",
    "    cbar.ax.set_yticklabels(class_names)\n",
    "    plt.clim(-0.5, len(class_names)-0.5)\n",
    "\n",
    "    plt.subplot(1, ncols, idx + 1)\n",
    "    overlay_pred = overlay_mask_on_image(image_array, pred_mask, class_names, alpha=0.5)\n",
    "    plt.imshow(overlay_pred)\n",
    "    plt.title(\"Image + Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Prediksi dan visualisasi\n",
    "def predict_and_visualize(model, image_path, input_size=(256, 256), class_names=None, true_mask=None):\n",
    "    img = load_img(image_path, target_size=input_size)\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_input = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    pred_mask = model.predict(img_input)[0]  # (H, W, C)\n",
    "    pred_classes = np.argmax(pred_mask, axis=-1)  # (H, W)\n",
    "\n",
    "    visualize_segmentation(img_array, pred_classes, true_mask=true_mask, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19137c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133dc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_true_mask(mask_path, target_size):\n",
    "    mask = load_img(mask_path, target_size=target_size, color_mode=\"grayscale\")\n",
    "    mask_array = img_to_array(mask).squeeze().astype(np.uint8)  # shape: (H, W)\n",
    "    return mask_array\n",
    "\n",
    "# Direktori image dan mask\n",
    "image_dir = f\"{ROOT_DIR}/test/images\"\n",
    "mask_dir = f\"{ROOT_DIR}/test/masks\"\n",
    "\n",
    "for img_file in os.listdir(image_dir):\n",
    "    if img_file.endswith(\".jpg\") or img_file.endswith(\".png\"):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "\n",
    "        # Ganti ekstensi ke .png jika mask disimpan dalam .png\n",
    "        mask_file = os.path.splitext(img_file)[0] + \".png\"\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        # Cek apakah file mask ada\n",
    "        if os.path.exists(mask_path):\n",
    "            true_mask = load_true_mask(mask_path, target_size=(256, 256))\n",
    "        else:\n",
    "            print(f\"WARNING: Mask not found for {img_file}, skipping true mask overlay.\")\n",
    "            true_mask = None\n",
    "\n",
    "        print(f\"Processing: {img_file}\")\n",
    "        predict_and_visualize(model_transunet_unet, img_path, input_size=(256, 256),\n",
    "                              class_names=CLASS_NAMES + [\"Background\"], true_mask=true_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84c643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
